{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "def read_geocsv(*args, **kwargs):\n",
    "    df = pd.read_csv(*args, **kwargs)    \n",
    "    df[\"geometry\"] = [wkt.loads(s) for s in df[\"geometry\"]] \n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    gdf.crs = {'init': 'epsg:4326'}\n",
    "    return gdf\n",
    "gpd.read_geocsv = read_geocsv\n",
    "\n",
    "def feature_to_maps_link(row):\n",
    "    centroid = row.centroid\n",
    "    return \"http://www.google.com/maps/place/%f,%f\" % (centroid.y, centroid.x)\n",
    "  \n",
    "def two_layer_map(top_layer, bottom_layer, column=None):\n",
    "    ax = bottom_layer.plot(figsize=(10, 8), column=column, legend=(column != None))\n",
    "    return top_layer.plot(ax=ax, color='pink', alpha=0.5, edgecolor=\"black\")\n",
    "\n",
    "def compute_area(gdf):\n",
    "    gdf.crs = {'init': 'epsg:4326'}\n",
    "    return gdf.to_crs(epsg=3395).area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redo the intersection to only keep maz_id from the right hand dataframe\n",
    "This saves parcels which intersect with more than one maz to a csv called joined.csv.  We can then read joined.csv in the next cell and not have to run this cell (which takes a while) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: CRS does not match!\n"
     ]
    }
   ],
   "source": [
    "def intersect(lower_gdf, upper_gdf):\n",
    "    return gpd.sjoin(lower_gdf, upper_gdf, how=\"inner\", op='intersects')\n",
    " \n",
    "parcels = gpd.read_geocsv(\"parcels.csv\")\n",
    "mazs = gpd.read_geocsv(\"mazs.csv\")\n",
    "mazs = gpd.GeoDataFrame(mazs[[\"maz_id\", \"geometry\"]])\n",
    "joined = intersect(parcels, mazs)\n",
    "overlaps = joined.apn.value_counts().loc[lambda x: x > 1]\n",
    "joined.drop(\"index_right\", axis=1)[joined.apn.isin(overlaps.index)].to_csv(\"joined.csv\", index=False)\n",
    "non_overlaps = joined.apn.value_counts().loc[lambda x: x == 1]\n",
    "joined.drop(\"index_right\", axis=1)[joined.apn.isin(non_overlaps.index)].to_csv(\"parcels_and_mazs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data to do parcel splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mazs = gpd.read_geocsv(\"mazs.csv\").set_index(\"maz_id\").drop([\"Shape_Area\", \"Shape_Leng\"], axis=1)\n",
    "joined = gpd.read_geocsv(\"joined.csv\").set_index(\"apn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for parcel splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_slivers_back_to_shapes(shapes, slivers):\n",
    "    for label, row in slivers.iterrows():\n",
    "        distances = [\n",
    "            row.geometry.distance(row2.geometry)\n",
    "            for _, row2 in shapes.iterrows()\n",
    "        ]\n",
    "        min_ind = np.argmin(distances)\n",
    "        closest_shape = shapes.iloc[min_ind]\n",
    "        closest_index = shapes.index[min_ind]\n",
    "\n",
    "        union = closest_shape.geometry.union(row.geometry)\n",
    "        shapes = shapes.set_value(closest_index, \"geometry\", union)\n",
    "\n",
    "    return shapes\n",
    "\n",
    "def compute_pct_area(df, total_area):\n",
    "    df[\"calc_area\"] = compute_area(df).values\n",
    "    df[\"pct_area\"] = df[\"calc_area\"] / total_area \n",
    "    return df\n",
    "    \n",
    "def split_parcel(parcel, split_shapes, dont_split_pct_cutoff=.01, proportional_fields=[], drop_not_in_maz=False):\n",
    "    try:\n",
    "        overlay = gpd.overlay(parcel, split_shapes.reset_index(), how='identity')\n",
    "    except:\n",
    "        print \"Parcel failed\"\n",
    "        return\n",
    "\n",
    "    overlay = compute_pct_area(overlay, compute_area(parcel).sum())\n",
    "\n",
    "    # now we need to make sure we don't split off very small portions of the parcel\n",
    "    split = overlay[overlay.pct_area >= dont_split_pct_cutoff].copy()\n",
    "    dont_split = overlay[overlay.pct_area < dont_split_pct_cutoff]\n",
    "    \n",
    "    split = merge_slivers_back_to_shapes(split, dont_split)\n",
    "    \n",
    "    if drop_not_in_maz:\n",
    "        split = split[~split.maz_id.isnull()]\n",
    "    \n",
    "    # have to recompute merge of slivers\n",
    "    split = compute_pct_area(split, compute_area(split).sum())\n",
    "    \n",
    "    # divvy these fields up by the percent area\n",
    "    for fld in proportional_fields:\n",
    "        split[fld] *= split.pct_area\n",
    "    \n",
    "    return split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do parcel splits\n",
    "This creates a file called split.csv which contains all the split geometries.  This file is read in the next cell and so cells before this point don't have to be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apn_counts = joined.index.value_counts()\n",
    "bad_apns = [\"999 999999999\"]\n",
    "proportional_fields = [\"bldg_sqft\", \"impr_val\", \"land_val\", \"nres_sqft\", \"res_units\"]\n",
    "\n",
    "print time.ctime()\n",
    "\n",
    "cnt = 0\n",
    "new_parcels = []\n",
    "for apn, _ in apn_counts.iteritems():\n",
    "    if apn in bad_apns: continue\n",
    "    subset = joined.loc[apn]\n",
    "    ret = split_parcel(subset.head(1).drop(\"maz_id\", axis=1), mazs[mazs.index.isin(subset.maz_id)],\n",
    "                       proportional_fields=[], drop_not_in_maz=True, dont_split_pct_cutoff=.03)\n",
    "    if ret is None: continue\n",
    "    ret[\"orig_apn\"] = apn\n",
    "    # make a new unique apn when we split a parcel\n",
    "    ret[\"apn\"] = [apn + \"-\" + str(i+1) for i in range(len(ret))]\n",
    "    new_parcels.append(ret)\n",
    "    cnt += 1\n",
    "    if cnt % 100 == 0: print \"Done %d of %d\" % (cnt, len(apn_counts))\n",
    "\n",
    "new_parcels = pd.concat(new_parcels)\n",
    "new_parcels.to_csv(\"split.csv\", index=False)\n",
    "print time.ctime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read split parcels and merge with parcels which don't have intersections\n",
    "(drop parcels which have been split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_parcels = gpd.read_geocsv(\"split.csv\", index_col=\"apn\")\n",
    "parcels = gpd.read_geocsv(\"parcels_and_mazs.csv\", index_col=\"apn\")\n",
    "parcels[\"orig_apn\"] = parcels.index\n",
    "split_parcels = gpd.GeoDataFrame(\n",
    "    pd.concat([parcels[~parcels.index.isin(split_parcels.orig_apn)], split_parcels]))\n",
    "buildings = gpd.read_geocsv(\"buildings.csv\", low_memory=False)\n",
    "buildings[\"building_id_tmp\"] = buildings.index\n",
    "split_parcels.to_csv(\"split_parcels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we join buildings to split parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_buildings = gpd.sjoin(buildings, split_parcels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify overlaps of buildings and split parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnts = joined_buildings.index.value_counts().loc[lambda x: x > 1]\n",
    "overlaps = joined_buildings.loc[cnts.index].copy()\n",
    "print len(cnts)\n",
    "len(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_overlap_areas(overlaps, overlapees):\n",
    "    '''\n",
    "    After a spatial join is done, this computes the actual area of the overlap.\n",
    "    overlaps is the result of the spatial join (which contains geometry for the overlaper)\n",
    "    overlapees is the geometry of the right side of the join\n",
    "    the \"index_right\" column of overlaps should be the index of overlapees\n",
    "    '''\n",
    "    total_overlaps = len(overlaps)\n",
    "    cnt = 0\n",
    "    overlap_area = []\n",
    "    for index, overlap in overlaps.iterrows():\n",
    "        overlapee = overlapees.loc[overlap.index_right]\n",
    "        #ax = overlaper.head(1).plot(alpha=.5)\n",
    "        #overlapee.loc[overlaper.index_right].tail(1).plot(ax=ax, color=\"red\")\n",
    "        try:\n",
    "            overlap_poly = gpd.overlay(gpd.GeoDataFrame([overlap]), gpd.GeoDataFrame([overlapee]), how=\"intersection\")\n",
    "        except:\n",
    "            overlap_area.append(np.nan)\n",
    "            print \"Failed:\", index\n",
    "            continue\n",
    "        cnt += 1\n",
    "        if cnt % 25 == 0:\n",
    "            print \"Finished %d of %d\" % (cnt, total_overlaps)\n",
    "        if len(overlap_poly) == 0:\n",
    "            overlap_area.append(0)\n",
    "            continue\n",
    "        overlap_area.append(compute_area(overlap_poly).values[0])\n",
    "\n",
    "    return pd.Series(overlap_area, index=overlaps.index)\n",
    "\n",
    "print time.ctime()\n",
    "overlapping_areas = compute_overlap_areas(overlaps, split_parcels)\n",
    "print time.ctime()\n",
    "\n",
    "# write it out\n",
    "pd.DataFrame({\"overlapping_areas\": overlapping_areas}).to_csv(\"overlapping_areas.csv\")\n",
    "\n",
    "overlapping_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the max overlapping percent area for each building footprint - I mean, the percentage overlap for the parcel with which a building overlaps the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overlapping_area = pd.read_csv(\"overlapping_areas.csv\", index_col=\"index\").overlapping_areas\n",
    "overlaps[\"overlapping_area\"] = overlapping_area\n",
    "large_overlaps = overlaps[overlaps.overlapping_area.fillna(0) > .03].copy()\n",
    "overlapping_area = large_overlaps.overlapping_area\n",
    "overlapping_pct_area = overlapping_area / overlapping_area.groupby(overlapping_area.index).transform('sum')\n",
    "large_overlaps[\"overlapping_pct_area\"] = overlapping_pct_area\n",
    "max_overlapping_pct_area = overlapping_pct_area.groupby(overlapping_pct_area.index).max()\n",
    "large_overlaps[\"max_overlapping_pct_area\"] = max_overlapping_pct_area "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A pretty high proportion of building footprints touch at least two parcels - these are the \"overlaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(buildings)\n",
    "print len(joined_buildings.index.value_counts())\n",
    "print len(large_overlaps.index.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the building footprints which only match one parcel - we assign them to that parcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = joined_buildings.index.value_counts().loc[lambda x: x == 1]\n",
    "non_overlaps = joined_buildings.loc[s.index].copy()\n",
    "len(non_overlaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then take the building footprints which match to multiple parcels, but to one parcel greater than a given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold = .65\n",
    "overlaps_greater_than_threshold = large_overlaps.query(\"overlapping_pct_area >= %f\" % threshold)\n",
    "len(overlaps_greater_than_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problematic_overlaps = large_overlaps.query(\"max_overlapping_pct_area < %f\" % threshold)\n",
    "problematic_overlaps = problematic_overlaps.sort_values(by=\"max_overlapping_pct_area\", ascending=False)\n",
    "len(problematic_overlaps.index.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def are_these_same_parcels(parcel_overlaps):\n",
    "    # this looks to see if the data on the parcels looks like multiple buildings\n",
    "    # or whether it looks like a single building with 0's on the other parcels\n",
    "    def majority_zero_values(s):\n",
    "        return len(s[s == 0]) / float(len(s)) > .5\n",
    "\n",
    "    return majority_zero_values(parcel_overlaps.bldg_sqft) and\\\n",
    "           majority_zero_values(parcel_overlaps.nres_sqft) and\\\n",
    "           majority_zero_values(parcel_overlaps.res_units)\n",
    "\n",
    "def deal_with_problematic_overlap(index, building_overlaps, split_parcels):\n",
    "    area = compute_area(building_overlaps.head(1)).values[0]\n",
    "    # sliver threshold varies by size of the building, for small parcels we\n",
    "    # want to bias towards not splitting it up, for large building it might\n",
    "    # make sense to split it up more frequently\n",
    "    sliver_cutoff = .25 if area < 500 else .03\n",
    "    \n",
    "    title = \"\"\n",
    "    keep = building_overlaps\n",
    "    building_overlaps = building_overlaps.query(\"overlapping_pct_area > %f\" % sliver_cutoff)\n",
    "    if len(building_overlaps) == 0:\n",
    "        # no non-slivers, but there mostly look like apartment buildings, townhomes, and such\n",
    "        # just put all the footprints back in\n",
    "        building_overlaps = keep\n",
    "\n",
    "    parcel_overlaps = split_parcels.loc[building_overlaps.index_right]\n",
    "    \n",
    "    if len(building_overlaps) == 1:\n",
    "        title = \"Single parcel\"\n",
    "    elif are_these_same_parcels(parcel_overlaps):\n",
    "        title = \"Union parcels\"\n",
    "    else:\n",
    "        title = \"Split building\"\n",
    "        \n",
    "    return title, building_overlaps\n",
    "    \n",
    "problematic_overlaps[\"calc_area\"] = compute_area(problematic_overlaps)\n",
    "# drop small footprints (these are like storage sheds, believe it or not)\n",
    "print \"Dropping %d small footprints\" % \\\n",
    "    len(problematic_overlaps[problematic_overlaps.calc_area <= 200].index.value_counts())\n",
    "large_problematic_overlaps = problematic_overlaps[problematic_overlaps.calc_area > 200]\n",
    "\n",
    "fixes = {}\n",
    "cnt = 0\n",
    "total_cnt = len(large_problematic_overlaps.index.unique())\n",
    "for index in large_problematic_overlaps.index.unique():\n",
    "    cnt += 1\n",
    "    if cnt % 25 == 0:\n",
    "        print \"Finished %d of %d\" % (cnt, total_cnt)\n",
    "    overlap_type, building_overlaps = \\\n",
    "        deal_with_problematic_overlap(index, large_problematic_overlaps.loc[index],\n",
    "                                      split_parcels)\n",
    "    fixes.setdefault(overlap_type, [])\n",
    "    fixes[overlap_type].append(building_overlaps)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chopped_up_buildings = []\n",
    "cnt = 0\n",
    "total_cnt = len(fixes['Split building'])\n",
    "for building_sets in fixes['Split building']:\n",
    "    cnt += 1\n",
    "    if cnt % 25 == 0:\n",
    "        print \"Finished %d of %d\" % (cnt, total_cnt)\n",
    "    out = gpd.overlay(\n",
    "        # we go back to the original buildings set in order to drop the joined columns\n",
    "        buildings.loc[building_sets.index].head(1),\n",
    "        split_parcels.loc[building_sets.index_right].reset_index(),\n",
    "        how='intersection')\n",
    "    \n",
    "    # we're splitting up building footprints, so append \"-1\", \"-2\", \"-3\" etc.\n",
    "    out[\"building_id_tmp\"] = out.building_id_tmp.astype(\"string\").str.\\\n",
    "        cat(['-'+str(x) for x in range(1, len(out) + 1)])\n",
    "    \n",
    "    chopped_up_buildings.append(out)\n",
    "\n",
    "chopped_up_buildings = pd.concat(chopped_up_buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings_linked_to_parcels = gpd.GeoDataFrame(pd.concat([\n",
    "    non_overlaps,\n",
    "    overlaps_greater_than_threshold,\n",
    "    chopped_up_buildings,\n",
    "    pd.concat(fixes['Single parcel'])\n",
    "    # leaving out union parcels for now because they're more complicated\n",
    "]))\n",
    "\n",
    "# these are not quite the same, but they should be close\n",
    "# the 2nd number may be lower than the 1st because we drop lots of very small building footprints\n",
    "# then the number is larger because we split many building footprints on parcel boundaries\n",
    "# in the end, either one may be larger than the other\n",
    "print len(joined_buildings.index.value_counts())\n",
    "print len(buildings_linked_to_parcels)\n",
    "buildings_linked_to_parcels[\"apn\"] = buildings_linked_to_parcels.index_right\n",
    "buildings_linked_to_parcels = buildings_linked_to_parcels[list(buildings.columns) + [\"apn\"]]\n",
    "\n",
    "s = buildings_linked_to_parcels.apn.notnull()\n",
    "assert s.value_counts()[True] == len(s)\n",
    "\n",
    "buildings_linked_to_parcels.to_csv(\"buildings_linked_to_parcels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we work towards splitting the attribute up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parcels = gpd.read_geocsv(\"split_parcels.csv\", index_col=\"apn\")\n",
    "# this file contains mapping of blocks to mazs to tazs, but we want the maz to taz mapping\n",
    "maz_to_taz = pd.read_csv(\"GeogXWalk2010_Blocks_MAZ_TAZ.csv\").\\\n",
    "    drop_duplicates(subset=[\"MAZ_ORIGINAL\"]).set_index(\"MAZ_ORIGINAL\").TAZ_ORIGINAL\n",
    "parcels[\"taz_id\"] = parcels.maz_id.map(maz_to_taz)\n",
    "buildings_linked_to_parcels = gpd.read_geocsv(\n",
    "    \"buildings_linked_to_parcels.csv\", low_memory=False, index_col=\"building_id_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbox_east</th>\n",
       "      <th>bbox_north</th>\n",
       "      <th>bbox_south</th>\n",
       "      <th>bbox_west</th>\n",
       "      <th>geometry</th>\n",
       "      <th>place_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.234196</td>\n",
       "      <td>37.90669</td>\n",
       "      <td>37.835727</td>\n",
       "      <td>-122.368679</td>\n",
       "      <td>POLYGON ((-122.3686793 37.8695716, -122.366186...</td>\n",
       "      <td>Berkeley, Alameda County, California, United S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bbox_east  bbox_north  bbox_south   bbox_west  \\\n",
       "0 -122.234196    37.90669   37.835727 -122.368679   \n",
       "\n",
       "                                            geometry  \\\n",
       "0  POLYGON ((-122.3686793 37.8695716, -122.366186...   \n",
       "\n",
       "                                          place_name  \n",
       "0  Berkeley, Alameda County, California, United S...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import osmnx\n",
    "berkeley = osmnx.gdf_from_places([\"Berkeley, CA\"])\n",
    "berkeley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings_linked_to_parcels['building:levels'] = \\\n",
    "    pd.to_numeric(buildings_linked_to_parcels['building:levels'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400407\n",
      "27129\n"
     ]
    }
   ],
   "source": [
    "berkeley_parcels = gpd.sjoin(parcels, berkeley)\n",
    "print len(parcels)\n",
    "print len(berkeley_parcels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 250 of 26470\n",
      "Finished 500 of 26470\n",
      "Finished 750 of 26470\n",
      "Finished 1000 of 26470\n",
      "Finished 1250 of 26470\n",
      "Finished 1500 of 26470\n",
      "Finished 1750 of 26470\n",
      "Finished 2000 of 26470\n",
      "Finished 2250 of 26470\n",
      "Finished 2500 of 26470\n",
      "Finished 2750 of 26470\n",
      "Finished 3000 of 26470\n",
      "Finished 3250 of 26470\n",
      "Finished 3500 of 26470\n",
      "Finished 3750 of 26470\n",
      "Finished 4000 of 26470\n",
      "Finished 4250 of 26470\n",
      "Finished 4500 of 26470\n",
      "Finished 4750 of 26470\n",
      "Finished 5000 of 26470\n",
      "Finished 5250 of 26470\n",
      "Finished 5500 of 26470\n",
      "Finished 5750 of 26470\n",
      "Finished 6000 of 26470\n",
      "Finished 6250 of 26470\n",
      "Finished 6500 of 26470\n",
      "Finished 6750 of 26470\n",
      "Finished 7000 of 26470\n",
      "Finished 7250 of 26470\n",
      "Finished 7500 of 26470\n",
      "Finished 7750 of 26470\n",
      "Finished 8000 of 26470\n",
      "Finished 8250 of 26470\n",
      "Finished 8500 of 26470\n",
      "Finished 8750 of 26470\n",
      "Finished 9000 of 26470\n",
      "Finished 9250 of 26470\n",
      "Finished 9500 of 26470\n",
      "Finished 9750 of 26470\n",
      "Finished 10000 of 26470\n",
      "Finished 10250 of 26470\n",
      "Finished 10500 of 26470\n",
      "Finished 10750 of 26470\n",
      "Finished 11000 of 26470\n",
      "Finished 11250 of 26470\n",
      "Finished 11500 of 26470\n",
      "Finished 11750 of 26470\n",
      "Finished 12000 of 26470\n",
      "Finished 12250 of 26470\n",
      "Finished 12500 of 26470\n",
      "Finished 12750 of 26470\n",
      "Finished 13000 of 26470\n",
      "Finished 13250 of 26470\n",
      "Finished 13500 of 26470\n",
      "Finished 13750 of 26470\n",
      "Finished 14000 of 26470\n",
      "Finished 14250 of 26470\n",
      "Finished 14500 of 26470\n",
      "Finished 14750 of 26470\n",
      "Finished 15000 of 26470\n",
      "Finished 15250 of 26470\n",
      "Finished 15500 of 26470\n",
      "Finished 15750 of 26470\n",
      "Finished 16000 of 26470\n",
      "Finished 16250 of 26470\n",
      "Finished 16500 of 26470\n",
      "Finished 16750 of 26470\n",
      "Finished 17000 of 26470\n",
      "Finished 17250 of 26470\n",
      "Finished 17500 of 26470\n",
      "Finished 17750 of 26470\n",
      "Finished 18000 of 26470\n",
      "Finished 18250 of 26470\n",
      "Finished 18500 of 26470\n",
      "Finished 18750 of 26470\n",
      "Finished 19000 of 26470\n",
      "Finished 19250 of 26470\n",
      "Finished 19500 of 26470\n",
      "Finished 19750 of 26470\n",
      "Finished 20000 of 26470\n",
      "Finished 20250 of 26470\n",
      "Finished 20500 of 26470\n",
      "Finished 20750 of 26470\n",
      "Finished 21000 of 26470\n",
      "Finished 21250 of 26470\n",
      "Finished 21500 of 26470\n",
      "Finished 21750 of 26470\n",
      "Finished 22000 of 26470\n",
      "Finished 22250 of 26470\n",
      "Finished 22500 of 26470\n",
      "Finished 22750 of 26470\n",
      "Finished 23000 of 26470\n",
      "Finished 23250 of 26470\n",
      "Finished 23500 of 26470\n",
      "Finished 23750 of 26470\n",
      "Finished 24000 of 26470\n",
      "Finished 24250 of 26470\n",
      "Finished 24500 of 26470\n",
      "Finished 24750 of 26470\n",
      "Finished 25000 of 26470\n",
      "Finished 25250 of 26470\n",
      "Finished 25500 of 26470\n",
      "Finished 25750 of 26470\n",
      "Finished 26000 of 26470\n",
      "Finished 26250 of 26470\n"
     ]
    }
   ],
   "source": [
    "def drop_parcel_attributes(parcels):\n",
    "    # used in two place so make a function\n",
    "    return parcels[['county_id', 'geometry', 'maz_id', 'taz_id', 'orig_apn']]\n",
    "\n",
    "def assign_parcel_attributes_to_buildings(buildings, parcels):\n",
    "    # attributes of the first parcel get applied to the buildings - if\n",
    "    # we did this right, the attributes of all subparcels will be the\n",
    "    # same - we pass in the parcels in order to set the schema for all\n",
    "    # the parcels.  test this a little bit:\n",
    "    for col in ['bldg_sqft', 'res_units', 'nres_sqft']:\n",
    "        assert len(parcels) == 1 or parcels[col].fillna(0).describe()['std'] == 0\n",
    "    # take attributes of the first parcel\n",
    "    parcel = parcels.iloc[0]\n",
    "        \n",
    "    # drop address and amenity - they're great columns but infrequently used\n",
    "    buildings = buildings[['name', 'geometry', 'apn', 'building:levels', 'building']]\n",
    "    buildings = buildings.rename(columns={'building:levels': 'stories', 'building': 'osm_building_type'})\n",
    "    buildings['calc_area'] = compute_area(buildings).round()\n",
    "    \n",
    "    # we call a building a shed if it's less than 50 meters large and it\n",
    "    # doesn't get any of the parcel data\n",
    "    sheds = buildings[buildings.calc_area < 80].copy()\n",
    "    sheds[\"small_building\"] = True\n",
    "    non_sheds = buildings[buildings.calc_area >= 80].copy()\n",
    "    non_sheds[\"small_building\"] = False\n",
    "    \n",
    "    non_sheds[\"stories\"] = non_sheds.stories.fillna(parcel.stories).fillna(1)\n",
    "    non_sheds[\"year_built\"] = parcel.year_built\n",
    "    non_sheds[\"building_type\"] = parcel.dev_type\n",
    "    \n",
    "    # account for height\n",
    "    built_area = non_sheds.calc_area * non_sheds.stories.astype('float')\n",
    "    # get built area proportion in each building footprint\n",
    "    proportion_built_area = built_area / built_area.sum()\n",
    "    \n",
    "    non_sheds[\"building_sqft\"] = (proportion_built_area * parcel.bldg_sqft).round()\n",
    "    non_sheds[\"residential_units\"] = (proportion_built_area * parcel.res_units).round()\n",
    "    non_sheds[\"non_residential_sqft\"] = (proportion_built_area * parcel.nres_sqft).round()\n",
    "    \n",
    "    return pd.concat([sheds, non_sheds]), drop_parcel_attributes(parcels)\n",
    "\n",
    "def make_dummy_building(parcels):\n",
    "    # when there's more than one parcel, we put the dummy building on the\n",
    "    # biggest sub parcel\n",
    "    parcel = parcels.sort_values(by=\"calc_area\", ascending=False).head(1)\n",
    "\n",
    "    if parcels.bldg_sqft.fillna(0).sum() == 0 and parcels.res_units.fillna(0).sum() == 0:\n",
    "        # there's no reason to make a dummy building if the attributes aren't there\n",
    "        return pd.DataFrame(), drop_parcel_attributes(parcels)\n",
    "\n",
    "    parcel.crs = {'init': 'epsg:4326'}\n",
    "    parcel = parcel.to_crs(epsg=3857) # switch to meters\n",
    "    circle = parcel.centroid.buffer(15).values[0] # buffer a circle in meters\n",
    "    parcel = parcel.to_crs(epsg=4326) # back to lat-lng\n",
    "    building = gpd.GeoDataFrame({\n",
    "        'name': ['Generated from parcel centroid'],\n",
    "        'geometry': [circle],\n",
    "        'apn': [parcel.index[0]],\n",
    "        'building:levels': [1],\n",
    "        'building': ['yes']\n",
    "    })\n",
    "    building.crs = {'init': 'epsg:3857'}\n",
    "    building = building.to_crs(epsg=4326)\n",
    "    return assign_parcel_attributes_to_buildings(building, parcels)\n",
    "\n",
    "new_buildings_list = []\n",
    "new_parcel_list = []\n",
    "\n",
    "cnt = 0\n",
    "filtered_parcels = berkeley_parcels\n",
    "#filtered_parcels = parcels[parcels.taz_id == 300419]\n",
    "#s = parcels.orig_apn.value_counts()[lambda x: x > 1]\n",
    "#filtered_parcels = parcels[parcels.orig_apn.isin(s.index)].iloc[:1000]\n",
    "\n",
    "grps = filtered_parcels.groupby(\"orig_apn\")\n",
    "total_cnt = len(grps)\n",
    "# iterate over parcels (not sub-parcels)\n",
    "for index, shared_apn_parcels in grps:\n",
    "    # get all buildngs that are on any subparcel of this parcel\n",
    "    buildings = buildings_linked_to_parcels[\n",
    "        buildings_linked_to_parcels.apn.isin(shared_apn_parcels.index)]\n",
    "    \n",
    "    if len(buildings) == 0:\n",
    "        new_buildings, new_parcels = make_dummy_building(shared_apn_parcels)\n",
    "    else:\n",
    "        new_buildings, new_parcels = assign_parcel_attributes_to_buildings(\n",
    "            gpd.GeoDataFrame(buildings), shared_apn_parcels)\n",
    "\n",
    "    new_buildings_list.append(new_buildings)\n",
    "    new_parcel_list.append(new_parcels)\n",
    "\n",
    "    cnt += 1\n",
    "    if cnt % 250 == 0:\n",
    "        print \"Finished %d of %d\" % (cnt, total_cnt)\n",
    "\n",
    "\n",
    "new_parcels = pd.concat(new_parcel_list)\n",
    "new_buildings = pd.concat(new_buildings_list)\n",
    "\n",
    "new_parcels.to_csv(\"moved_attribute_parcels.csv\")\n",
    "new_buildings.to_csv(\"moved_attribute_buildings.csv\")\n",
    "\n",
    "open(\"test_parcels.geojson\", \"w\").write(new_parcels.to_json())\n",
    "open(\"test_buildings.geojson\", \"w\").write(new_buildings.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation below this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(fixes['Union parcels'])\n",
    "for parcel_sets in fixes['Union parcels'][10:11]:\n",
    "    print feature_to_maps_link(parcel_sets.head(1))\n",
    "    print parcel_sets.head(1).name\n",
    "    two_layer_map(parcel_sets, split_parcels.loc[parcel_sets.index_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apns = new_parcels.apn.unique()\n",
    "new_parcels[new_parcels.apn == apns[0]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_parcels[new_parcels.apn == apns[1]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_parcels[new_parcels.apn == apns[2]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_parcels[new_parcels.apn == apns[3]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings = gpd.read_geocsv(\"buildings.csv\", low_memory=False)\n",
    "neighborhoods = gpd.read_geocsv(\"ca_neighborhoods.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downtown = neighborhoods[neighborhoods.City == \"Oakland\"].query(\"Name == 'Downtown'\")\n",
    "broadmoor = neighborhoods[neighborhoods.City == \"San Leandro\"].query(\"Name == 'Broadmoor'\")\n",
    "#downtown_buildings = gpd.sjoin(buildings, downtown)\n",
    "broadmoor_buildings = gpd.sjoin(buildings, broadmoor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parcels = gpd.read_geocsv(\"parcels.csv\")\n",
    "#downtown_parcels = gpd.sjoin(parcels, downtown)\n",
    "broadmoor_parcels = gpd.sjoin(parcels, broadmoor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = broadmoor_parcels.plot(color='red', figsize=(50, 50))\n",
    "broadmoor_buildings.plot(ax=ax, color='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighborhoods[neighborhoods.City == \"San Leandro\"]\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parcel_building_intersections = intersect(buildings, parcels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(parcel_building_intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = parcel_building_intersections.apn.value_counts()\n",
    "s = s[s > 1]\n",
    "print len(s)\n",
    "apn = s.index[0]\n",
    "print apn\n",
    "c = parcels[parcels.apn == apn].centroid.geometry.values[0]\n",
    "print c.y, c.x\n",
    "ax = parcels[parcels.apn == apn].plot(color='red', figsize=(50, 50))\n",
    "parcel_building_intersections[parcel_building_intersections.apn == apn].plot(ax=ax, color='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
