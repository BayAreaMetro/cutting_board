{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "def read_geocsv(*args, **kwargs):\n",
    "    df = pd.read_csv(*args, **kwargs)    \n",
    "    df[\"geometry\"] = [wkt.loads(s) for s in df[\"geometry\"]] \n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    gdf.crs = {'init': 'epsg:4326'}\n",
    "    return gdf\n",
    "gpd.read_geocsv = read_geocsv\n",
    "\n",
    "def feature_to_maps_link(row):\n",
    "    centroid = row.centroid\n",
    "    return \"http://www.google.com/maps/place/%f,%f\" % (centroid.y, centroid.x)\n",
    "  \n",
    "def two_layer_map(top_layer, bottom_layer, column=None):\n",
    "    ax = bottom_layer.plot(figsize=(10, 8), column=column, legend=(column != None))\n",
    "    return top_layer.plot(ax=ax, color='pink', alpha=0.5, edgecolor=\"black\")\n",
    "\n",
    "def compute_area(gdf):\n",
    "    gdf.crs = {'init': 'epsg:4326'}\n",
    "    return gdf.to_crs(epsg=3395).area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redo the intersection to only keep maz_id from the right hand dataframe\n",
    "This saves parcels which intersect with more than one maz to a csv called joined.csv.  We can then read joined.csv in the next cell and not have to run this cell (which takes a while) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: CRS does not match!\n"
     ]
    }
   ],
   "source": [
    "def intersect(lower_gdf, upper_gdf):\n",
    "    return gpd.sjoin(lower_gdf, upper_gdf, how=\"inner\", op='intersects')\n",
    " \n",
    "parcels = gpd.read_geocsv(\"parcels.csv\")\n",
    "mazs = gpd.read_geocsv(\"mazs.csv\")\n",
    "mazs = gpd.GeoDataFrame(mazs[[\"maz_id\", \"geometry\"]])\n",
    "joined = intersect(parcels, mazs)\n",
    "overlaps = joined.apn.value_counts().loc[lambda x: x > 1]\n",
    "joined.drop(\"index_right\", axis=1)[joined.apn.isin(overlaps.index)].to_csv(\"joined.csv\", index=False)\n",
    "non_overlaps = joined.apn.value_counts().loc[lambda x: x == 1]\n",
    "joined.drop(\"index_right\", axis=1)[joined.apn.isin(non_overlaps.index)].to_csv(\"parcels_and_mazs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data to do parcel splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mazs = gpd.read_geocsv(\"mazs.csv\").set_index(\"maz_id\").drop([\"Shape_Area\", \"Shape_Leng\"], axis=1)\n",
    "joined = gpd.read_geocsv(\"joined.csv\").set_index(\"apn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for parcel splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_slivers_back_to_shapes(shapes, slivers):\n",
    "    for label, row in slivers.iterrows():\n",
    "        distances = [\n",
    "            row.geometry.distance(row2.geometry)\n",
    "            for _, row2 in shapes.iterrows()\n",
    "        ]\n",
    "        min_ind = np.argmin(distances)\n",
    "        closest_shape = shapes.iloc[min_ind]\n",
    "        closest_index = shapes.index[min_ind]\n",
    "\n",
    "        union = closest_shape.geometry.union(row.geometry)\n",
    "        shapes = shapes.set_value(closest_index, \"geometry\", union)\n",
    "\n",
    "    return shapes\n",
    "\n",
    "def compute_pct_area(df, total_area):\n",
    "    df[\"calc_area\"] = compute_area(df).values\n",
    "    df[\"pct_area\"] = df[\"calc_area\"] / total_area \n",
    "    return df\n",
    "    \n",
    "def split_parcel(parcel, split_shapes, dont_split_pct_cutoff=.01, proportional_fields=[], drop_not_in_maz=False):\n",
    "    try:\n",
    "        overlay = gpd.overlay(parcel, split_shapes.reset_index(), how='identity')\n",
    "    except:\n",
    "        print \"Parcel failed\"\n",
    "        return\n",
    "\n",
    "    overlay = compute_pct_area(overlay, compute_area(parcel).sum())\n",
    "\n",
    "    # now we need to make sure we don't split off very small portions of the parcel\n",
    "    split = overlay[overlay.pct_area >= dont_split_pct_cutoff].copy()\n",
    "    dont_split = overlay[overlay.pct_area < dont_split_pct_cutoff]\n",
    "    \n",
    "    split = merge_slivers_back_to_shapes(split, dont_split)\n",
    "    \n",
    "    if drop_not_in_maz:\n",
    "        split = split[~split.maz_id.isnull()]\n",
    "    \n",
    "    # have to recompute merge of slivers\n",
    "    split = compute_pct_area(split, compute_area(split).sum())\n",
    "    \n",
    "    # divvy these fields up by the percent area\n",
    "    for fld in proportional_fields:\n",
    "        split[fld] *= split.pct_area\n",
    "    \n",
    "    return split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do parcel splits\n",
    "This creates a file called split.csv which contains all the split geometries.  This file is read in the next cell and so cells before this point don't have to be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apn_counts = joined.index.value_counts()\n",
    "bad_apns = [\"999 999999999\"]\n",
    "proportional_fields = [\"bldg_sqft\", \"impr_val\", \"land_val\", \"nres_sqft\", \"res_units\"]\n",
    "\n",
    "print time.ctime()\n",
    "\n",
    "cnt = 0\n",
    "new_parcels = []\n",
    "for apn, _ in apn_counts.iteritems():\n",
    "    if apn in bad_apns: continue\n",
    "    subset = joined.loc[apn]\n",
    "    ret = split_parcel(subset.head(1).drop(\"maz_id\", axis=1), mazs[mazs.index.isin(subset.maz_id)],\n",
    "                       proportional_fields=[], drop_not_in_maz=True, dont_split_pct_cutoff=.03)\n",
    "    if ret is None: continue\n",
    "    ret[\"orig_apn\"] = apn\n",
    "    # make a new unique apn when we split a parcel\n",
    "    ret[\"apn\"] = [apn + \"-\" + str(i+1) for i in range(len(ret))]\n",
    "    new_parcels.append(ret)\n",
    "    cnt += 1\n",
    "    if cnt % 100 == 0: print \"Done %d of %d\" % (cnt, len(apn_counts))\n",
    "\n",
    "new_parcels = pd.concat(new_parcels)\n",
    "new_parcels.to_csv(\"split.csv\", index=False)\n",
    "print time.ctime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read split parcels and merge with parcels which don't have intersections\n",
    "(drop parcels which have been split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_parcels = gpd.read_geocsv(\"split.csv\", index_col=\"apn\")\n",
    "parcels = gpd.read_geocsv(\"parcels_and_mazs.csv\", index_col=\"apn\")\n",
    "parcels[\"orig_apn\"] = parcels.index\n",
    "split_parcels = gpd.GeoDataFrame(\n",
    "    pd.concat([parcels[~parcels.index.isin(split_parcels.orig_apn)], split_parcels]))\n",
    "buildings = gpd.read_geocsv(\"buildings.csv\", low_memory=False)\n",
    "buildings[\"building_id_tmp\"] = buildings.index\n",
    "split_parcels.to_csv(\"split_parcels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we join buildings to split parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_buildings = gpd.sjoin(buildings, split_parcels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify overlaps of buildings and split parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = joined_buildings.index.value_counts().loc[lambda x: x > 1]\n",
    "overlaps = joined_buildings.loc[cnts.index].copy()\n",
    "print len(cnts)\n",
    "len(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_overlap_areas(overlaps, overlapees):\n",
    "    '''\n",
    "    After a spatial join is done, this computes the actual area of the overlap.\n",
    "    overlaps is the result of the spatial join (which contains geometry for the overlaper)\n",
    "    overlapees is the geometry of the right side of the join\n",
    "    the \"index_right\" column of overlaps should be the index of overlapees\n",
    "    '''\n",
    "    total_overlaps = len(overlaps)\n",
    "    cnt = 0\n",
    "    overlap_area = []\n",
    "    for index, overlap in overlaps.iterrows():\n",
    "        overlapee = overlapees.loc[overlap.index_right]\n",
    "        #ax = overlaper.head(1).plot(alpha=.5)\n",
    "        #overlapee.loc[overlaper.index_right].tail(1).plot(ax=ax, color=\"red\")\n",
    "        try:\n",
    "            overlap_poly = gpd.overlay(gpd.GeoDataFrame([overlap]), gpd.GeoDataFrame([overlapee]), how=\"intersection\")\n",
    "        except:\n",
    "            overlap_area.append(np.nan)\n",
    "            print \"Failed:\", index\n",
    "            continue\n",
    "        cnt += 1\n",
    "        if cnt % 25 == 0:\n",
    "            print \"Finished %d of %d\" % (cnt, total_overlaps)\n",
    "        if len(overlap_poly) == 0:\n",
    "            overlap_area.append(0)\n",
    "            continue\n",
    "        overlap_area.append(compute_area(overlap_poly).values[0])\n",
    "\n",
    "    return pd.Series(overlap_area, index=overlaps.index)\n",
    "\n",
    "print time.ctime()\n",
    "overlapping_areas = compute_overlap_areas(overlaps, split_parcels)\n",
    "print time.ctime()\n",
    "\n",
    "# write it out\n",
    "pd.DataFrame({\"overlapping_areas\": overlapping_areas}).to_csv(\"overlapping_areas.csv\")\n",
    "\n",
    "overlapping_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the max overlapping percent area for each building footprint - I mean, the percentage overlap for the parcel with which a building overlaps the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_area = pd.read_csv(\"overlapping_areas.csv\", index_col=\"index\").overlapping_areas\n",
    "overlaps[\"overlapping_area\"] = overlapping_area\n",
    "large_overlaps = overlaps[overlaps.overlapping_area.fillna(0) > .03].copy()\n",
    "overlapping_area = large_overlaps.overlapping_area\n",
    "overlapping_pct_area = overlapping_area / overlapping_area.groupby(overlapping_area.index).transform('sum')\n",
    "large_overlaps[\"overlapping_pct_area\"] = overlapping_pct_area\n",
    "max_overlapping_pct_area = overlapping_pct_area.groupby(overlapping_pct_area.index).max()\n",
    "large_overlaps[\"max_overlapping_pct_area\"] = max_overlapping_pct_area "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A pretty high proportion of building footprints touch at least two parcels - these are the \"overlaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(buildings)\n",
    "print len(joined_buildings.index.value_counts())\n",
    "print len(large_overlaps.index.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the building footprints which only match one parcel - we assign them to that parcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = joined_buildings.index.value_counts().loc[lambda x: x == 1]\n",
    "non_overlaps = joined_buildings.loc[s.index].copy()\n",
    "len(non_overlaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then take the building footprints which match to multiple parcels, but to one parcel greater than a given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold = .65\n",
    "overlaps_greater_than_threshold = large_overlaps.query(\"overlapping_pct_area >= %f\" % threshold)\n",
    "len(overlaps_greater_than_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_overlaps = large_overlaps.query(\"max_overlapping_pct_area < %f\" % threshold)\n",
    "problematic_overlaps = problematic_overlaps.sort_values(by=\"max_overlapping_pct_area\", ascending=False)\n",
    "len(problematic_overlaps.index.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def are_these_same_parcels(parcel_overlaps):\n",
    "    # this looks to see if the data on the parcels looks like multiple buildings\n",
    "    # or whether it looks like a single building with 0's on the other parcels\n",
    "    def majority_zero_values(s):\n",
    "        return len(s[s == 0]) / float(len(s)) > .5\n",
    "\n",
    "    return majority_zero_values(parcel_overlaps.bldg_sqft) and\\\n",
    "           majority_zero_values(parcel_overlaps.nres_sqft) and\\\n",
    "           majority_zero_values(parcel_overlaps.res_units)\n",
    "\n",
    "def deal_with_problematic_overlap(index, building_overlaps, split_parcels):\n",
    "    area = compute_area(building_overlaps.head(1)).values[0]\n",
    "    # sliver threshold varies by size of the building, for small parcels we\n",
    "    # want to bias towards not splitting it up, for large building it might\n",
    "    # make sense to split it up more frequently\n",
    "    sliver_cutoff = .25 if area < 500 else .03\n",
    "    \n",
    "    title = \"\"\n",
    "    keep = building_overlaps\n",
    "    building_overlaps = building_overlaps.query(\"overlapping_pct_area > %f\" % sliver_cutoff)\n",
    "    if len(building_overlaps) == 0:\n",
    "        # no non-slivers, but there mostly look like apartment buildings, townhomes, and such\n",
    "        # just put all the footprints back in\n",
    "        building_overlaps = keep\n",
    "\n",
    "    parcel_overlaps = split_parcels.loc[building_overlaps.index_right]\n",
    "    \n",
    "    if len(building_overlaps) == 1:\n",
    "        title = \"Single parcel\"\n",
    "    elif are_these_same_parcels(parcel_overlaps):\n",
    "        title = \"Union parcels\"\n",
    "    else:\n",
    "        title = \"Split building\"\n",
    "        \n",
    "    return title, building_overlaps\n",
    "    \n",
    "problematic_overlaps[\"calc_area\"] = compute_area(problematic_overlaps)\n",
    "# drop small footprints (these are like storage sheds, believe it or not)\n",
    "print \"Dropping %d small footprints\" % \\\n",
    "    len(problematic_overlaps[problematic_overlaps.calc_area <= 200].index.value_counts())\n",
    "large_problematic_overlaps = problematic_overlaps[problematic_overlaps.calc_area > 200]\n",
    "\n",
    "fixes = {}\n",
    "cnt = 0\n",
    "total_cnt = len(large_problematic_overlaps.index.unique())\n",
    "for index in large_problematic_overlaps.index.unique():\n",
    "    cnt += 1\n",
    "    if cnt % 25 == 0:\n",
    "        print \"Finished %d of %d\" % (cnt, total_cnt)\n",
    "    overlap_type, building_overlaps = \\\n",
    "        deal_with_problematic_overlap(index, large_problematic_overlaps.loc[index],\n",
    "                                      split_parcels)\n",
    "    fixes.setdefault(overlap_type, [])\n",
    "    fixes[overlap_type].append(building_overlaps)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chopped_up_buildings = []\n",
    "cnt = 0\n",
    "total_cnt = len(fixes['Split building'])\n",
    "for building_sets in fixes['Split building']:\n",
    "    cnt += 1\n",
    "    if cnt % 25 == 0:\n",
    "        print \"Finished %d of %d\" % (cnt, total_cnt)\n",
    "    out = gpd.overlay(\n",
    "        # we go back to the original buildings set in order to drop the joined columns\n",
    "        buildings.loc[building_sets.index].head(1),\n",
    "        split_parcels.loc[building_sets.index_right].reset_index(),\n",
    "        how='intersection')\n",
    "    \n",
    "    # we're splitting up building footprints, so append \"-1\", \"-2\", \"-3\" etc.\n",
    "    out[\"building_id_tmp\"] = out.building_id_tmp.astype(\"string\").str.\\\n",
    "        cat(['-'+str(x) for x in range(1, len(out) + 1)])\n",
    "    \n",
    "    chopped_up_buildings.append(out)\n",
    "\n",
    "chopped_up_buildings = pd.concat(chopped_up_buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_linked_to_parcels = gpd.GeoDataFrame(pd.concat([\n",
    "    non_overlaps,\n",
    "    overlaps_greater_than_threshold,\n",
    "    chopped_up_buildings,\n",
    "    pd.concat(fixes['Single parcel'])\n",
    "    # leaving out union parcels for now because they're more complicated\n",
    "]))\n",
    "\n",
    "# these are not quite the same, but they should be close\n",
    "# the 2nd number may be lower than the 1st because we drop lots of very small building footprints\n",
    "# then the number is larger because we split many building footprints on parcel boundaries\n",
    "# in the end, either one may be larger than the other\n",
    "print len(joined_buildings.index.value_counts())\n",
    "print len(buildings_linked_to_parcels)\n",
    "buildings_linked_to_parcels[\"apn\"] = buildings_linked_to_parcels.index_right\n",
    "buildings_linked_to_parcels = buildings_linked_to_parcels[list(buildings.columns) + [\"apn\"]]\n",
    "\n",
    "s = buildings_linked_to_parcels.apn.notnull()\n",
    "assert s.value_counts()[True] == len(s)\n",
    "\n",
    "buildings_linked_to_parcels.to_csv(\"buildings_linked_to_parcels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we work towards splitting the attribute up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300419"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maz_to_taz.loc[310641]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = gpd.read_geocsv(\"split_parcels.csv\", index_col=\"apn\")\n",
    "# this file contains mapping of blocks to mazs to tazs, but we want the maz to taz mapping\n",
    "maz_to_taz = pd.read_csv(\"GeogXWalk2010_Blocks_MAZ_TAZ.csv\").\\\n",
    "    drop_duplicates(subset=[\"MAZ_ORIGINAL\"]).set_index(\"MAZ_ORIGINAL\").TAZ_ORIGINAL\n",
    "parcels[\"taz_id\"] = parcels.maz_id.map(maz_to_taz)\n",
    "buildings_linked_to_parcels = gpd.read_geocsv(\n",
    "    \"buildings_linked_to_parcels.csv\", low_memory=False, index_col=\"building_id_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def assign_parcel_attributes_to_buildings(buildings, parcel):\n",
    "    # drop address and amenity - they're great columns but infrequently used\n",
    "    buildings = buildings[['name', 'geometry', 'apn', 'building:levels', 'building']]\n",
    "    buildings = buildings.rename(columns={'building:levels': 'stories', 'building': 'osm_building_type'})\n",
    "    buildings['calc_area'] = compute_area(buildings).round()\n",
    "    \n",
    "    # we call a building a shed if it's less than 50 meters large and it\n",
    "    # doesn't get any of the parcel data\n",
    "    sheds = buildings[buildings.calc_area < 80].copy()\n",
    "    sheds[\"small_building\"] = True\n",
    "    non_sheds = buildings[buildings.calc_area >= 80].copy()\n",
    "    non_sheds[\"small_building\"] = False\n",
    "    \n",
    "    non_sheds[\"stories\"] = non_sheds.stories.fillna(parcel.stories).fillna(1)\n",
    "    non_sheds[\"year_built\"] = parcel.year_built\n",
    "    non_sheds[\"building_type\"] = parcel.dev_type\n",
    "    \n",
    "    # account for height\n",
    "    built_area = non_sheds.calc_area * non_sheds.stories.astype('float')\n",
    "    # get built area proportion in each building footprint\n",
    "    proportion_built_area = built_area / built_area.sum()\n",
    "    \n",
    "    non_sheds[\"building_sqft\"] = (proportion_built_area * parcel.bldg_sqft).round()\n",
    "    non_sheds[\"residential_units\"] = (proportion_built_area * parcel.res_units).round()\n",
    "    non_sheds[\"non_residential_sqft\"] = (proportion_built_area * parcel.nres_sqft).round()\n",
    "    \n",
    "    parcel = parcel[['county_id', 'geometry', 'maz_id', 'taz_id', 'orig_apn']]\n",
    "    \n",
    "    return pd.concat([sheds, non_sheds]), parcel\n",
    "\n",
    "def make_dummy_building(parcel):\n",
    "    parcel = gpd.GeoDataFrame([parcel])\n",
    "    parcel.crs = {'init': 'epsg:4326'}\n",
    "    parcel = parcel.to_crs(epsg=3857) # switch to meters\n",
    "    circle = parcel.centroid.buffer(15).values[0] # buffer 10 meters\n",
    "    parcel = parcel.to_crs(epsg=4326)\n",
    "    building = gpd.GeoDataFrame({\n",
    "        'name': ['Generated from parcel centroid'],\n",
    "        'geometry': [circle],\n",
    "        'apn': [parcel.index[0]],\n",
    "        'building:levels': [1],\n",
    "        'building': ['yes']\n",
    "    })\n",
    "    building.crs = {'init': 'epsg:3857'}\n",
    "    building = building.to_crs(epsg=4326)\n",
    "    return assign_parcel_attributes_to_buildings(building, parcel.iloc[0])\n",
    "\n",
    "new_buildings_list = []\n",
    "new_parcel_list = []\n",
    "\n",
    "buildings_by_parcel = buildings_linked_to_parcels.groupby(\"apn\")\n",
    "\n",
    "# XXX parcels still have copied attributes when split on MAZ!\n",
    "\n",
    "# iterate over all buildings on each parcel\n",
    "cnt = 0\n",
    "#filtered_parcels = parcels[parcels.taz_id == 300419]\n",
    "filtered_parcels = parcels.iloc[0:10]\n",
    "\n",
    "total_cnt = len(filtered_parcels)\n",
    "for index, parcel in filtered_parcels.iterrows():\n",
    "    cnt += 1\n",
    "    if cnt % 250 == 0:\n",
    "        print \"Finished %d of %d\" % (cnt, total_cnt)    \n",
    "\n",
    "    try:\n",
    "        buildings = buildings_by_parcel.get_group(index)\n",
    "    except:\n",
    "        # parcel not found\n",
    "        buildings = None\n",
    "\n",
    "    if buildings is None:\n",
    "        new_buildings, new_parcel = make_dummy_building(parcel)\n",
    "    else:\n",
    "        new_buildings, new_parcel = assign_parcel_attributes_to_buildings(\n",
    "            gpd.GeoDataFrame(buildings), parcel)\n",
    "        \n",
    "    new_buildings_list.append(new_buildings)\n",
    "    new_parcel_list.append(new_parcel)\n",
    "\n",
    "new_parcels = gpd.GeoDataFrame(new_parcel_list)\n",
    "new_buildings = pd.concat(new_buildings_list)\n",
    "\n",
    "new_parcels.to_csv(\"moved_attribute_parcels.csv\")\n",
    "new_buildings.to_csv(\"moved_attribute_buildings.csv\")\n",
    "\n",
    "open(\"test_parcels.geojson\", \"w\").write(new_parcels.to_json())\n",
    "open(\"test_buildings.geojson\", \"w\").write(new_buildings.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation below this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(fixes['Union parcels'])\n",
    "for parcel_sets in fixes['Union parcels'][10:11]:\n",
    "    print feature_to_maps_link(parcel_sets.head(1))\n",
    "    print parcel_sets.head(1).name\n",
    "    two_layer_map(parcel_sets, split_parcels.loc[parcel_sets.index_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apns = new_parcels.apn.unique()\n",
    "new_parcels[new_parcels.apn == apns[0]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parcels[new_parcels.apn == apns[1]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parcels[new_parcels.apn == apns[2]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parcels[new_parcels.apn == apns[3]].plot(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings = gpd.read_geocsv(\"buildings.csv\", low_memory=False)\n",
    "neighborhoods = gpd.read_geocsv(\"ca_neighborhoods.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downtown = neighborhoods[neighborhoods.City == \"Oakland\"].query(\"Name == 'Downtown'\")\n",
    "broadmoor = neighborhoods[neighborhoods.City == \"San Leandro\"].query(\"Name == 'Broadmoor'\")\n",
    "#downtown_buildings = gpd.sjoin(buildings, downtown)\n",
    "broadmoor_buildings = gpd.sjoin(buildings, broadmoor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parcels = gpd.read_geocsv(\"parcels.csv\")\n",
    "#downtown_parcels = gpd.sjoin(parcels, downtown)\n",
    "broadmoor_parcels = gpd.sjoin(parcels, broadmoor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = broadmoor_parcels.plot(color='red', figsize=(50, 50))\n",
    "broadmoor_buildings.plot(ax=ax, color='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods[neighborhoods.City == \"San Leandro\"]\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parcel_building_intersections = intersect(buildings, parcels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parcel_building_intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = parcel_building_intersections.apn.value_counts()\n",
    "s = s[s > 1]\n",
    "print len(s)\n",
    "apn = s.index[0]\n",
    "print apn\n",
    "c = parcels[parcels.apn == apn].centroid.geometry.values[0]\n",
    "print c.y, c.x\n",
    "ax = parcels[parcels.apn == apn].plot(color='red', figsize=(50, 50))\n",
    "parcel_building_intersections[parcel_building_intersections.apn == apn].plot(ax=ax, color='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
